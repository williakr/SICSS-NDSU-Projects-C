---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(httr)
library(tidyjson)
```

Constructing the query
```{r}
api_key <- read_file("../reg-gov-api-key.txt")
docketID <- "DHS-2021-0015"
```

```{r}
reg_request <- GET(
  url = paste0("https://api.regulations.gov/v4/documents?",
               "filter[docketId]=", docketID,
               "&api_key=", api_key),
  config = config(ssl_verifypeer = FALSE)
)
```

Check that the request went through and check the url:
```{r}
http_status(reg_request)
reg_request$url
```

Get the objectId for each of the retrieved documents
```{r}
stringi::stri_enc_detect(content(reg_request, "raw")) # detect encoding
reg_json <- content(reg_request, "text", encoding = "ISO-8859-1") %>% # extract content
  as.tbl_json() # store json as tbl

# The following code extracts the objectId from the JSON for each document
# I pieced this together and don't fully know what each part does
reg_docs <- spread_values(reg_json) %>% 
  enter_object("data") %>% 
  gather_array() %>% 
  enter_object("attributes") %>%
  spread_values(objectId = jstring("objectId"))

```

Get comments for each document
```{r}
reg_comments_request <- list()
for (i in 1:nrow(reg_docs)) {
  reg_comments_request[[i]] <- GET(
    url = paste0("https://api.regulations.gov/v4/comments?",
                 "filter[commentOnId]=", reg_docs$objectId[i],
                 "&page[size]=220",
                 "&api_key=", api_key),
    config = config(ssl_verifypeer = FALSE)
  )
  Sys.sleep(time = 5)
}

reg_comments_request
```

Get comments from request
```{r}
reg_comments_request[[2]]$url
stringi::stri_enc_detect(content(reg_comments_request[[1]], "raw")) # detect encoding
# Save JSON as tbl
reg_comments_json <- content(reg_comments_request[[2]], "text", encoding = "ISO-8859-1") %>% as.tbl_json()

# Get comment IDs from JSON
reg_commentIDs <- spread_values(reg_comments_json) %>%
  enter_object("data") %>%
  gather_array() %>%
  spread_values(commentID = jstring("id"))

```

Request comment details
```{r}
reg_comments_details <- list()
for (i in 1:nrow(reg_commentIDs)) {
  reg_comments_details[[i]] <- GET(
    url = paste0("https://api.regulations.gov/v4/comments/",
                 reg_commentIDs$commentID[i],"?",
                 "include=attachments",
                 "&api_key=", api_key),
    config = config(ssl_verifypeer = FALSE)
  )
  Sys.sleep(time = 1.3)
}
beepr::beep("treasure")
```

```{r}
reg_comments_details[[220]]$url # check the request url
reg_comments_details[[1]]$url # check the request url

headers(reg_comments_details[[220]])$`x-ratelimit-remaining` # check the number of requests left this hour
```

Extract the text of the comment from the request
```{r}
# content(reg_comments_details[[1]], "text", encoding = "ISO-8859-1") %>%
#   as.tbl_json() %>%
#   spread_values(commentID = jstring("data","id"),
#                 commentTitle = jstring("data","attributes","title"),
#                 commentText = jstring("data","attributes","comment")) %>%
#   enter_object("included") %>%
#   gather_array(column.name = "included.index") %>%
#   spread_values(attachID = jstring("id"),
#                 attachments = jstring("type"))


comments_data <- map_dfr(reg_comments_details, function(x) {
  content(x, "text", encoding = "ISO-8859-1") %>%
  as.tbl_json() %>%
  spread_values(commentID = jstring("data","id"),
                commentTitle = jstring("data","attributes","title"),
                commentText = jstring("data","attributes","comment"))
})

comments_data$commentText

saveRDS(comments_data, file = "../data/comments_scraped.rds")
write_csv(comments_data, file = "../data/comments_scraped.csv")
```

```{r}
# Test reading the data into R
readRDS("data/comments_scraped.rds")
```

